---
title: "Predicting Stock Index Realised Volatility using Bayesian VARs"
author: "Nathan Huynh"

format:
  html:
    toc: true
    toc-location: left

execute:
  echo: false

knitr:
  opts_knit:
    root.dir: "C:\\Users\\Nathan\\Documents\\GitHub\\mcxs_report"

bibliography: references.bib
---

> **Abstract:** This research report explores whether VARs and Bayesian VARs are able to predict realised volatility in equity index markets.
>
> **Keywords.** BVARs, Realised Volatility, Stock Indices, SP500

# Research Proposal

## Objective and Motivation

This paper seeks to examine the effectiveness of Bayesian VARs as a method for forecasting realised volatility (**RV**) in equity markets. It will explore whether various Bayesian estimation techniques applied to vector autoregression can accurately predict RV. The accurate prediction of market volatility has many applications in Finance, including the pricing of derivatives and the estimation of risk measures such as Value at Risk.

## Data

The models will be applied to daily data on realised variances occurring on a group of global stock indices. Stock indices serve as gauges of overall equity market performance and are generally categorised by country. For this analysis we will focus on 10 major global indices, the SPX, DJI, FTSE, GDAXI, FCHI, STOXX50E, N225, AORD, HSI and the KS11.

| Code     | Index Name    | Region   |
|----------|---------------|----------|
| SPX      | S&P 500       | US       |
| DJI      | Dow Jones     | US       |
| FTSE     | FTSE 100      | UK       |
| GDAXI    | DAX           | Germany  |
| FCHI     | CAC40         | France   |
| STOXX50E | EURO STOXX    | Europe   |
| N225     | NIKKEI 225    | Japan    |
| AORD     | All Ordinaries| Australia|
| HSI      | Hang Seng     | Hong Kong|
| KS11     | KOSPI         | Korea    | 

Realised variance is a measure of historical volatility occuring in financial time series constructed from intraday high frequency return data.

Realised variance is defined as the sum of squared returns over specific period:
```{=tex}
\begin{align}
Realised\,Variance = \sum r^2_t
\end{align}
```
Where $p_t$ denotes the price of an asset at time t and $r_t$ is defined as the log return over a predetermined interval, for example 5 minutes:
```{=tex}
\begin{align}
r_t = log(p_t / p_{t-1})
\end{align}
```
Realised volatility **RV** is then computed as the square root of the realised variance.
```{=tex}
\begin{align}
RV = \sqrt{\sum r^2_t}
\end{align}
```

The RV data is sourced from the [Oxford Man Realised Library](https://oxford-man.ox.ac.uk/) which provides a number of precalculated volatility metrics, including RV on stock indices spanning multiple years. The data set has been employed widely in the literatur for empirical volatility studies, such as by Dutta and Das ([2022](https://onlinelibrary.wiley.com/doi/full/10.1002/fut.22372)) and Brandi and Di Matteo ([2022](https://arxiv.org/pdf/2201.10466.pdf)). For this analysis we will utilise the 5 minute RV measure provided in the dataset for each of our 10 indices. The data ranges between January 2000 and June 2018.

##### Figure 1. Time series plots of original values {style="text-align: center;"}
```{R loading in data}

#Sourcing RV data from the Oxford Man Realized Library

data = read.csv('https://github.com/nathanh93/mcxs_report/raw/master/data/oxfordmanrealizedvolatilityindices.csv')


#Putting the RV data in a dataframe

SPX = na.omit(xts::xts(data[data$Symbol=='.SPX', 'rv5'],as.Date(data[data$Symbol=='.SPX', 'X'])))
FTSE = na.omit(xts::xts(data[data$Symbol=='.FTSE', 'rv5'],as.Date(data[data$Symbol=='.FTSE', 'X'])))
DAX = na.omit(xts::xts(data[data$Symbol=='.GDAXI', 'rv5'],as.Date(data[data$Symbol=='.GDAXI', 'X'])))
NIKKEI = na.omit(xts::xts(data[data$Symbol=='.N225', 'rv5'],as.Date(data[data$Symbol=='.N225', 'X'])))

df <- as.data.frame(
  merge(SPX, 
        FTSE, 
        DAX, 
        NIKKEI))

colnames(df) <- c("S&P Daily RV","FTSE Daily RV","DAX Daily RV","Nikkei Daily RV")
dates <- as.Date(rownames(df),format = "%Y-%m-%d")
titles <- c("S&P Daily RV","FTSE Daily RV","DAX Daily RV","Nikkei Daily RV")

#Plotting the original values of 4 indices 

par(mfrow=c(2,2), mar=c(3,3,2,2))
for (i in 1:ncol(df)){
  plot(dates, y = df[,i], type = "l", 
       main = titles[i], ylab = "RV", xlab = "Date",
       col = 'blue',
       ylim = c(min(na.omit(df[,i])),max(na.omit(df[,i]))))
}
```
# 

As each of the series follow a similar general pattern only four of the ten indices are plotted for visualisation above. From a visual inspection of the examples above, we can see that realised volatility appears highly non-stationary. There are clear spikes in RV over certain periods in time. These periods of high volatility appear to persist for some time before subsiding, which provides evidence in favour of an autoregressive model specification.

Furthermore, spikes in RV appear to happen around the same times across markets. This provides our main motivation for modelling RV via a VAR specification whereby we can seek to capture the dynamic interrelationships between global equity markets.

For ease of modelling and in order to bring the data closer to normality we will work with the log transformed variable $log(rv_t)$.

##### Figure 2. Time series plots of log transformed values {style="text-align: center;"}
```{r}
#Storing log transformed variables in a dataframe

df.log <- as.data.frame(
  merge(log(SPX), 
        log(FTSE), 
        log(DAX), 
        log(NIKKEI)))
colnames(df.log) <- c("S&P log RV","FTSE log RV","DAX Daily RV","Nikkei log RV")
dates <- as.Date(rownames(df.log),format = "%Y-%m-%d")
new_titles <- c("S&P log RV","FTSE log RV","DAX log RV","Nikkei log RV")

#Plotting the log transformed values of 4 indices

par(mfrow=c(2,2), mar=c(3,3,2,2))
for (i in 1:ncol(df.log)){
  plot(dates, y = df.log[,i], type = "l", ylab = "RV", xlab = "Date",
       main = new_titles[i],
       col = 'blue'
       )
}

```
#

Conducting Augmented Dickey-Fuller tests on the log transformed RVs to test for stationarity results in the rejection of the null hypothesis of non-stationarity for all series at the 0.05 level. This implies stationarity in the log transformed variables.


##### ADF Test Results for Log Transformed 5 Minute RVs {style="text-align: center;"}
```{r, warning=FALSE, echo=FALSE, message=FALSE}

df = as.data.frame(matrix(nrow=0, ncol=2))
colnames(df) = c('Index','P Value')

for (i in c('.SPX', '.DJI', '.FTSE', '.GDAXI','.FCHI','.STOXX50E','.N225','.AORD','.HSI','.KS11')){
  testres = tseries::adf.test(log(data[data$Symbol==i,'rv5']))
  p = testres$p.value
  new = c(i, p)
  df[nrow(df)+1, ] = new
}

formattable::formattable(df)

```

## Model

The model will follow the standard $VAR(p)$ setup as follows:

```{=tex}
\begin{align}
rv_t &= \mu_0 + A_1 rv_{t-1} + \dots + A_p rv_{t-p} + \epsilon_t\\
\epsilon_t | RV_{t-1} &\sim iidN_N(0_N, \Sigma)
\end{align}
```

Where $rv_t$ is a vector of log transformed realised variances for our $N=10$ stock indices on day $t$. The $A_i$ matrices are $N\times N$ matrices of the autoregressive slope parameters.

The error term vector $\epsilon_t$ given the data up to $t-1$ is assumed to be iid multivariate normally distributed of dimension $N=10$, with mean $0_N$ and covariance matrix $\Sigma$.

Bayesian estimation techniques will be then utilised in conjunction with a number of suitably chosen prior specifications in order to estimate competing models and compute 1 day ahead RV forecasts across all indices. The predictions will be made out of sample and the accuracy of the forecasts will then be assessed according to their root mean squared errors **RMSE**, given by:

```{=tex}
\begin{align}
RMSE = \sqrt{\sum(\hat{rv_i} - rv_i )^2/n}
\end{align}
```

Assessment of prediction accuracy via the RMSEs will facilitate comparison of various different model specifications, such as the incorporation of different prior distributions and differing assumptions about the parameters of those distributions.

The significance of being able to reliably forecast market volatility is primarily seen in the context of financial asset pricing. Volatility of the underlying asset is one of the crucial inputs required in options pricing. With reliable forecasts of stock index volatility one can assess the degree to which options quoted in the market are under or over estimating volatility relative to what is indicated by the historical realised dynamics.



# Modelling Framework

## Estimation Procedure for the Baseline Model

The baseline model is as follows:

```{=tex}
\begin{align}
Y &= XA + U \\
Y|X,A,\Sigma &\sim MN_{T \times N} (XA, \Sigma, I_T)
\end{align}
```

This implies the following form for the kernel of the likelihood function:

```{=tex}
\begin{align}
L(A,\Sigma|Y,X) \propto det(\Sigma)^{-\frac{T}{2}}exp(-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)])
\end{align}
```

We assume the usual matrix normal and inverse Wishart natural conjugate priors for $A$ and $\Sigma$:

```{=tex}
\begin{align}
p(A,\Sigma) &= p(A|\Sigma)p(\Sigma) \\
A|\Sigma &\sim MN_{K \times N}(\underline{A},\Sigma,\underline{V}) \\
\Sigma &\sim IW_N(\underline{S},\underline{\nu})
\end{align}
```

The posterior distribution is given by the product of the likelihood and the priors.

```{=tex}
\begin{align}
p(A,\Sigma|Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \\
&\times exp(-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)]) \\
&\times \det(\Sigma)^{-\frac{N+K+\underline{v}+1}{2}} \\
&\times exp(-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A})'\underline(V)^{-1}(A-\underline{A})]) \\
&\times exp(-\frac{1}{2}tr[\Sigma^{-1}\underline{S}])
\end{align}
```


Combining the terms and completing the squares for the terms within the square brackets yields the following posterior distributions for $A$ and $\Sigma$:

```{=tex}
\begin{align}
p(A|Y,X,\Sigma) &= MN_{K \times N}(\bar{A}, \Sigma, \bar{V}) \\
p(\Sigma|Y,X) &= IW_N(\bar{S},\bar{\nu}) \\
\\
\bar{V} &= (X'X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu} \\
\bar{S} &= \underline{S} + Y'Y + \underline{A}'\underline{V}^{-1}\underline{A} - \bar{A}'\bar{V}^{-1}\bar{A} \\

\end{align}
```

Since the above fully characterises the joint posterior distribution we can then sample from these directly using a suitably chosen prior. A common prior for unit root nonstationary variables is the 'Minnesota Prior', which assumes that the variables follow a random walk. The prior mean $\underline{A}$ is set to an identity matrix for the first lag and zeroes elsewhere. Meanwhile the column specific covariance matrix for A is set according to two shrinkage hyperparameters $\kappa_1$ and $\kappa_2$ which determine the level of shrinkage around the prior mean.

As the ADF tests for the log transformed RVs suggest that they are stationary, I instead work with an alternate form of the 'Minnesota Prior' whereby the variables are characterised as  'white noise'. To achieve this, the $\underline{A}$ matrix is set to a $K \times N$ matrix of zeroes.

```{=tex}
\begin{align}
\underline{A} &= 0_{K \times N} \\

\underline{V} &= diag( \begin{bmatrix} \kappa_2 & \kappa_1(p^{-2}\otimes I_N') \end{bmatrix})

\end{align}
```

The estimation routine can be implemented in a function in R as follows:

### Posterior Estimation Function in R
```{r, echo=TRUE}
get_posteriors = function(S, Y, X, priors) {
  N = ncol(Y)
  
  A.prior     = priors[[1]]
  V.prior     = priors[[2]]
  S.prior     = priors[[3]]
  nu.prior    = priors[[4]]
    
  # normal-inverse Wishart posterior parameters
  ############################################################
  V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))
  V.bar       = solve(V.bar.inv)
  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
  nu.bar      = nrow(Y) + nu.prior
  S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   = solve(S.bar)
  
  
  # posterior draws 
  ############################################################
  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior   = apply(Sigma.posterior,3,solve)
  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
  L                 = t(chol(V.bar))
  for (s in 1:S){
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
  }
  
  return(list(A.posterior, Sigma.posterior))
}

```

For demonstrative purposes the above code can be applied to simulated data generated from a bivariate random walk.

```{r}
rw_data = data.frame(matrix(nrow=1000, ncol=2))
rw_data[,1] = cumsum(rnorm(1000,0,1))
rw_data[,2] = cumsum(rnorm(1000,0,1))
plot(rw_data[,1], type='l', ylim=c(min(rw_data), max(rw_data)), col='red', ylab='', xlab='', main='Bivariate Random Walk')
lines(rw_data[,2], col='blue', ylab='', xlab='')
```


The estimation routine utilising a Minnesota Prior specification is then implemented as follows:

```{r, echo=TRUE}
p=1
N=2

Y_ext = (rw_data[(p+1):nrow(rw_data),c(1,2)]) #removing first p observations and taking logs
X_ext = matrix(1,nrow(Y_ext),1)                                       #Creating X matrix
for (i in 1:p){
  X_ext     = cbind(X_ext, (rw_data[(p+1):nrow(rw_data)-i,c(1,2)]))
}

Y_ext = as.matrix(Y_ext)
X_ext = as.matrix(X_ext)

# A.hat       = solve(t(X_ext)%*%X_ext)%*%t(X_ext)%*%Y_ext
# Sigma.hat   = t(Y_ext-X_ext%*%A.hat)%*%(Y_ext-X_ext%*%A.hat)/nrow(Y_ext)
# 
# kappa.1     = 0.02^2
# kappa.2     = 100
# A.pri     = matrix(0, (1+p*N), N)
# A.pri[2:3,] = diag(2)
# V.pri     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
# S.pri     = diag(diag(Sigma.hat))
# nu.pri    = N+1

Prior <-function(N,P,X_ext, Y_ext){
        A.hat       = solve(t(X_ext)%*%X_ext)%*%t(X_ext)%*%Y_ext
        Sigma.hat   = t(Y_ext-X_ext%*%A.hat)%*%(Y_ext-X_ext%*%A.hat)/nrow(Y_ext)
        
        kappa.1     = 0.02^2
        kappa.2     = 100
        A.pri     = matrix(0, (1+p*N), N)
        A.pri[2:3,] = diag(2)
        V.pri     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
        S.pri     = diag(diag(Sigma.hat))
        nu.pri    = N+1
  
        return(list(A.prior = A.pri,
                    V.prior = V.pri,
                    S.prior = S.pri,
                    nu.prior= nu.pri
                    ))
}

pris = list(A.pri, V.pri, S.pri, nu.pri)

res = get_posteriors(10000, Y_ext, X_ext, pris)

round(apply(res[[1]],1:2,mean),2) #A posterior mean
round(apply(res[[2]],1:2,mean),2) #Sigma posterior mean

```

As shown above the estimated parameter means closely match the expected values for the data generating process.

## Model Extension: T-Distributed Errors

An alternative specification for the model is to relax the assumption of normally distributed errors. A T-Distribution more closely mirrors the leptokurtosis commonly seen in financial time series and as such it is a good candidate for our model of index volatility.

```{=tex}
\begin{align}
U_t &\sim t_N(0, \Sigma, \nu) 
\end{align}
```

Following the methodology of [Geweke 1993](https://onlinelibrary.wiley.com/doi/epdf/10.1002/jae.3950080504?saml_referrer), a T-distribution for the error term can be represented by a scale mixture of normal distributions with a scaling term $\lambda$ which is Inverse Gamma 2 distributed.

```{=tex}
\begin{align}
U|\lambda &\sim MN(0, \Sigma, \lambda I_t) \\
\lambda &\sim IG2( s_{\lambda}, \nu_{\lambda}) 
\end{align}
```

Under this specification, the kernel of the likelihood function takes the following form:

```{=tex}
\begin{align}

L(A,\Sigma,\Lambda|Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \det(\lambda I_T)^{-\frac{N}{2}} exp(-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' (\lambda I_T)^{-1} (Y-XA) ])

\end{align}
```

The posteriors for $A$, $\Sigma$ and $\lambda$ can then be derived using the likelihood and the prior distributions. The natural conjugacy of $A$ and $\Sigma$ is preserved and so the conditional posterior $p(A,\Sigma|Y,X)$ can be derived as follows.

```{=tex}
\begin{align}
p(A,\Sigma|Y,X) &\propto L(A,\Sigma,\lambda|Y,X)p(A,\Sigma) \\
\\
&= \det(\Sigma)^{-\frac{T}{2}} \det(\lambda I_T)^{-\frac{N}{2}} exp(-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' (\lambda I_T)^{-1} (Y-XA) ]) \\
&\times \det(\Sigma)^{-\frac{N+k+\underline{\nu}+1}{2}} exp(-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A})'(\underline{V})^{-1}(A-\underline{A})]) \\
&\times exp(-\frac{1}{2}tr[\Sigma^{-1}\underline{S}]) \\
\\
&= \det(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}} \det(\lambda I_T)^{-\frac{N}{2}} \\

&\times exp(-\frac{1}{2} tr[\Sigma^{-1}(Y'(\lambda I_T)^{-1}Y - 2A'X'(\lambda I_T)^{-1}Y + A'X'(\lambda I_T)^{-1}XA + A'\underline{V}^{-1}A -2A'\underline{V}^{-1}\underline{A} + \underline{A}'\underline{V}^{-1}\underline{A} + \underline{S})])


\end{align}
```

Expanding the terms inside the square brackets, followed by completing the squares, allows the above expression to be rearranged in the form of a Matrix-variate Normal Inverse Wishart kernel.

```{=tex}
\begin{align}

p(A,\Sigma|Y,X) &\sim MNIW(\bar{A},\bar{V},\bar{S},\bar{\nu}) \\
\\
\bar{V} &= (X'(\lambda I_T)^{-1}X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'(\lambda I_T)^{-1}Y + \underline{V}^{-1}\underline{A}) \\
\bar{S} &= Y'(\lambda I_T)^{-1}Y + \underline{A}'\underline{V}^{-1}\underline{A} + \underline{S} - \bar{A}'\bar{V}^{-1}\bar{A} \\
\bar{\nu} &= T + \underline{\nu}

\end{align}
```

The posterior distribution for $\lambda$ is then derived as follows:

```{=tex}
\begin{align}

p(\lambda|Y,X,A,\Sigma) &\propto L(A,\Sigma,\lambda|Y,X)p(\lambda) \\
\\
&= \det(\Sigma)^{-\frac{T}{2}} \det(\lambda I_T)^{-\frac{N}{2}} exp(-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' (\lambda I_T)^{-1} (Y-XA) ]) \\
&\times \lambda^{-\frac{\underline{\nu_{\lambda}}+2}{2}} exp(-\frac{1}{2}\frac{\underline{s_{\lambda}}}{\lambda}) \\
\\
&= \lambda^{-\frac{TN}{2}} exp(-\frac{1}{2}\frac{1}{\lambda} tr[\Sigma^{-1}(Y-XA)'(Y-XA)]) \\
&\times \lambda^{-\frac{\underline{\nu_{\lambda}}+2}{2}} exp(-\frac{1}{2}\frac{\underline{s_{\lambda}}}{\lambda}) \det(\Sigma)^{-\frac{T}{2}}\det(I_T)^{-\frac{N}{2}} \\
\\
&= \lambda^{-\frac{TN+\underline{\nu_{\lambda}}+2}{2}} exp(-\frac{1}{2}\frac{1}{\lambda} [tr[\Sigma^{-1}(Y-XA)'(Y-XA)] +\underline{s_{\lambda}}]) \det(\Sigma)^{-\frac{T}{2}} \\
\\
\end{align}
```

As such,

```{=tex}
\begin{align}

\lambda|Y,A,\Sigma &\sim IG2(\bar{s_{\lambda}},\bar{\nu_{\lambda}}) \\
\bar{s_{\lambda}} &= tr[\Sigma^{-1}(Y-XA)'(Y-XA)] + \underline{s_{\lambda}} \\
\bar{\nu_{\lambda}} &= TN + \underline{\nu_{\lambda}}

\end{align}
```

We can then sample sequentially from the conditional posterior distributions via a Gibbs Sampler. In order to do so we initalise each of the parameters at arbitrary starting values. We then proceed accordingly:

1. Draw $\Sigma^{(i)}$ from the $IW(\bar{S},\bar{\nu})$ distribution
2. Draw $A^{(i)}$ from the $MN(\bar{A},\Sigma^{(i)}, \bar{V})$ distribution
3. Draw $\lambda^{(i)}$ from $IG2(\bar{s_{\lambda}},\bar{\nu_{\lambda}})$

We repeat the above steps for the desired amount of iterations and collect the parameter draws in an array. We can then scrutinise the characteristics of the posterior densities using these draws.

### Gibbs Sampler code for t-distributed errors:
```{r, echo=TRUE}

# #Setting priors and initialising parameters
# A.gprior = matrix(0, 3, 2)
# A_V.gprior = diag(1,3)
# Sigma_s.gprior = diag(2)
# Sigma_v.gprior = 3
# lambda_s.gprior = 1000
# lambda_v.gprior = 1000
# 
# lambda.draw = lambda_s.gprior/rchisq(1, lambda_v.gprior)
# 
# S = 1000
# 
# Sigma.posterior.draws = array(NA, c(2,2,S))
# A.posterior.draws = array(NA, c(3,2,S))
# lambda.posterior.draws = rep(NA,S)
# for (s in 1:S){
#   
#   lambda.gprior.diag = diag(lambda.draw, nrow(Y_ext))
#   
#   A_V.posterior     = solve(t(X_ext)%*%diag(1/diag(lambda.gprior.diag))%*%X_ext + solve(A_V.gprior))
#   A.posterior       = A_V.posterior%*%(t(X_ext)%*%diag(1/diag(lambda.gprior.diag))%*%Y_ext + solve(A_V.gprior)%*%A.gprior)
#   Sigma_s.posterior = t(Y_ext)%*%diag(1/diag(lambda.gprior.diag))%*%Y_ext + t(A.gprior)%*%solve(A_V.gprior)%*%A.gprior + Sigma_s.gprior - t(A.posterior)%*%solve(A_V.posterior)%*%A.posterior
#   Sigma_v.posterior = nrow(Y_ext) + Sigma_v.gprior
#   
#   Sigma.inv.draw      = rWishart(1, Sigma_v.posterior, solve(Sigma_s.posterior))[,,1]
#   Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)
#   
#   A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.posterior), sigma=Sigma.posterior.draws[,,s]%x%A_V.posterior), ncol=2)
#   
#   lambda_s.posterior = sum(diag(Sigma.inv.draw%*%t(Y_ext - X_ext%*%A.posterior.draws[,,s])%*%(Y_ext - X_ext%*%A.posterior.draws[,,s]))) + lambda_s.gprior
#   lambda_v.posterior = nrow(Y_ext)*2 + lambda_v.gprior
#   
#   lambda.draw = lambda_s.posterior / rchisq(1, lambda_v.posterior)
#   lambda.posterior.draws[s] = lambda.draw
# }

GS_tdistribution <- function(Y_ext, X_ext, S = 1000) {
  # Set priors
  A.gprior = matrix(0, 3, 2)
  A_V.gprior = diag(1,3)
  Sigma_s.gprior = diag(2)
  Sigma_v.gprior = 3
  lambda_s.gprior = 1000
  lambda_v.gprior = 1000

  lambda.draw = lambda_s.gprior/rchisq(1, lambda_v.gprior)
  
  # Initialize arrays to store posterior draws
  Sigma.posterior.draws = array(NA, c(2,2,S))
  A.posterior.draws = array(NA, c(3,2,S))
  lambda.posterior.draws = rep(NA,S)

  # Loop over S iterations
  for (s in 1:S){
    lambda.gprior.diag = diag(lambda.draw, nrow(Y_ext))
    
    A_V.posterior     = solve(t(X_ext)%*%diag(1/diag(lambda.gprior.diag))%*%X_ext + solve(A_V.gprior))
    A.posterior       = A_V.posterior%*%(t(X_ext)%*%diag(1/diag(lambda.gprior.diag))%*%Y_ext + solve(A_V.gprior)%*%A.gprior)
    Sigma_s.posterior = t(Y_ext)%*%diag(1/diag(lambda.gprior.diag))%*%Y_ext + t(A.gprior)%*%solve(A_V.gprior)%*%A.gprior + Sigma_s.gprior - t(A.posterior)%*%solve(A_V.posterior)%*%A.posterior
    Sigma_v.posterior = nrow(Y_ext) + Sigma_v.gprior
    
    Sigma.inv.draw      = rWishart(1, Sigma_v.posterior, solve(Sigma_s.posterior))[,,1]
    Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)
    
    A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.posterior), sigma=Sigma.posterior.draws[,,s]%*%A_V.posterior), ncol=2)
    
    lambda_s.posterior = sum(diag(Sigma.inv.draw%*%t(Y_ext - X_ext%*%A.posterior.draws[,,s])%*%(Y_ext - X_ext%*%A.posterior.draws[,,s]))) + lambda_s.gprior
    lambda_v.posterior = nrow(Y_ext)*2 + lambda_v.gprior
    
    lambda.draw = lambda_s.posterior / rchisq(1, lambda_v.posterior)
    lambda.posterior.draws[s] = lambda.draw
  }
  return(list(Sigma.posterior.draws = Sigma.posterior.draws, 
              A.posterior.draws = A.posterior.draws, 
              lambda.posterior.draws = lambda.posterior.draws))
}

round(apply(A.posterior.draws, 1:2, mean),2)
round(apply(Sigma.posterior.draws, 1:2, mean),2)
round(mean(lambda.posterior.draws),2)

```

As can be seen, when the extended model estimation routine is applied to the simulated bivariate random walk data, the estimates of $A$ and $\Sigma$ again match the true parameter values closely.


# Empirical Results

The estimation procedures outlined above can be readily applied to the log RV data using the previously stated 'Minnesota Prior'. For the purposes of this estimation I apply a lag of 5 to capture 1 week of trading. The estimated parameter matrices for $A$ and $\Sigma$ for both the baseline and extended models are as follows:

```{r, echo=FALSE}
#Arranging the data into Y and X Matrices

data_sample = data[data$Symbol %in% c('.SPX','.DJI','.FTSE','.GDAXI','.FCHI', '.STOXX50E', '.N225', '.AORD', '.HSI', '.KS11'),]
data_sample['Date'] = as.Date(data_sample$X)

y = reshape(data_sample[,c('Date','Symbol','rv5')], idvar='Date', timevar='Symbol',direction='wide' )
colnames(y) = c('Date','.AORD','.DJI','.FCHI','.FTSE','.GDAXI','.HSI','.KS11','.N225','.SPX','.STOXX50E')
y = y[order(y$Date),]
y = na.omit(y)


p=5
N=10

Y_ext = log(y[(p+1):nrow(y),c('.AORD','.DJI','.FCHI','.FTSE','.GDAXI','.HSI','.N225','.SPX','.KS11','.STOXX50E')])
X_ext = matrix(1,nrow(Y_ext),1)
for (i in 1:p){
X_ext     = cbind(X_ext, log(y[(p+1):nrow(y)-i,c('.AORD','.DJI','.FCHI','.FTSE','.GDAXI','.HSI','.N225','.SPX','.KS11','.STOXX50E')]))
}

Y_ext = as.matrix(Y_ext)
X_ext = as.matrix(X_ext)

```

### Baseline Model Estimation
```{r, echo=TRUE}

# A.hat       = solve(t(X_ext)%*%X_ext)%*%t(X_ext)%*%Y_ext
# Sigma.hat   = t(Y_ext-X_ext%*%A.hat)%*%(Y_ext-X_ext%*%A.hat)/T
# 
# kappa.1     = 0.02^2
# kappa.2     = 100
# A.pri       = matrix(0, (1+p*N), N)
# V.pri       = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
# S.pri       = diag(diag(Sigma.hat))
# nu.pri      = N+1
# 
# 
# pris = list(A.pri, V.pri, S.pri, nu.pri)

pris = Prior(N,P,X_ext, Y_ext)
res = get_posteriors(1000, Y_ext, X_ext, pris)

head(round(apply(res[[1]],1:2,mean),2)) #A posterior mean
head(round(apply(res[[2]],1:2,mean),2)) #Sigma posterior mean

```


### Gibbs Sampler Estimation of Extended Model
```{r, echo=TRUE}

#Setting priors and initialising parameters
A.gprior = matrix(0, (1+p*N), N)
A_V.gprior = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
Sigma_s.gprior = diag(diag(Sigma.hat))
Sigma_v.gprior = N+1
lambda_s.gprior = 4
lambda_v.gprior = 4

lambda.draw = lambda_s.gprior/rchisq(1, lambda_v.gprior)

S = 1000 # A limited number of draws are chosen for speed of estimation. This will be increased in the final submission.

Sigma.posterior.draws = array(NA, c(N,N,S))
A.posterior.draws = array(NA, c((1+p*N),N,S))
lambda.posterior.draws = rep(NA,S)

for (s in 1:S){
  
  lambda.gprior.diag = diag(lambda.draw, nrow(Y_ext))
  
  A_V.posterior     = solve(t(X_ext)%*%diag(1/diag(lambda.gprior.diag))%*%X_ext + solve(A_V.gprior))
  A.posterior       = A_V.posterior%*%(t(X_ext)%*%diag(1/diag(lambda.gprior.diag))%*%Y_ext + solve(A_V.gprior)%*%A.gprior)
  Sigma_s.posterior = t(Y_ext)%*%diag(1/diag(lambda.gprior.diag))%*%Y_ext + t(A.gprior)%*%solve(A_V.gprior)%*%A.gprior + Sigma_s.gprior - t(A.posterior)%*%solve(A_V.posterior)%*%A.posterior
  Sigma_v.posterior = nrow(Y_ext) + Sigma_v.gprior
  
  Sigma.inv.draw      = rWishart(1, Sigma_v.posterior, solve(Sigma_s.posterior))[,,1]
  Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)
  
  A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.posterior), sigma=Sigma.posterior.draws[,,s]%x%A_V.posterior), ncol=2)
  
  lambda_s.posterior = sum(diag(Sigma.inv.draw%*%t(Y_ext - X_ext%*%A.posterior.draws[,,s])%*%(Y_ext - X_ext%*%A.posterior.draws[,,s]))) + lambda_s.gprior
  lambda_v.posterior = nrow(Y_ext)*2 + lambda_v.gprior
  
  lambda.draw = lambda_s.posterior / rchisq(1, lambda_v.posterior)
  lambda.posterior.draws[s] = lambda.draw
}
                            
head(round(apply(A.posterior.draws, 1:2, mean),2))
head(round(apply(Sigma.posterior.draws, 1:2, mean),2))
round(mean(lambda.posterior.draws),2)

```


We can then compute forecasts for both models by sampling from the predictive densities. For each draw of $A$ and $\Sigma$, we sample h times from the predictive density in order to compute a forecast horizon of h periods. Each of the h draws are computed as 1 period ahead forecasts. Where the predictive density requires data which has not yet been observed this is replaced by the preceding forecasts computed within the same iteration.


```{r, echo=TRUE, warning=FALSE, message=FALSE}
#Predictive density function

library(MASS)
library(HDInterval)
library(mvtnorm)
library(plot3D)

get_predictions = function(S, h, A.posterior, Sigma.posterior){
  Y.h         = array(NA,c(h,N,S))
  
  for (s in 1:(S/1)){
    x.Ti        = Y_ext[(nrow(Y_ext)-p+1):nrow(Y_ext),]
    if (p>1){
      x.Ti        = x.Ti[1:p,] 
    }
    for (i in 1:h){
      x.T         = c(1,as.vector(t(x.Ti)))
      Y.h[i,,s]   = rmvnorm(1, mean = x.T%*%A.posterior[,,s], sigma=Sigma.posterior[,,s])
      if (p==1){
        x.Ti        = Y.h[i,,s]
      } else {
        x.Ti        = rbind(Y.h[i,,s],x.Ti[1:(p-1),])
      }
    }
  }
  
  point.f.1 = apply(Y.h[,1,],1,mean)
  point.interval.1 = apply(Y.h[,1,],1,hdi,credMass=0.90)
  
  point.f.2 = apply(Y.h[,2,],1,mean)
  point.interval.2 = apply(Y.h[,2,],1,hdi,credMass=0.90)
  
  point.f.3 = apply(Y.h[,3,],1,mean)
  point.interval.3 = apply(Y.h[,3,],1,hdi,credMass=0.90)

  point.f.4 = apply(Y.h[,4,],1,mean)
  point.interval.4 = apply(Y.h[,4,],1,hdi,credMass=0.90)
  
  point.f.5 = apply(Y.h[,5,],1,mean)
  point.interval.5 = apply(Y.h[,5,],1,hdi,credMass=0.90)
  
  point.f.6 = apply(Y.h[,6,],1,mean)
  point.interval.6 = apply(Y.h[,6,],1,hdi,credMass=0.90)
  
  point.f.7 = apply(Y.h[,7,],1,mean)
  point.interval.7 = apply(Y.h[,7,],1,hdi,credMass=0.90)
  
  point.f.8 = apply(Y.h[,8,],1,mean)
  point.interval.8 = apply(Y.h[,8,],1,hdi,credMass=0.90)
  
  point.f.9 = apply(Y.h[,9,],1,mean)
  point.interval.9 = apply(Y.h[,9,],1,hdi,credMass=0.90)
  
  point.f.10 = apply(Y.h[,10,],1,mean)
  point.interval.10 = apply(Y.h[,10,],1,hdi,credMass=0.90)
  
  point.forecasts = list(point.f.1, point.f.2, point.f.3, point.f.4, point.f.5, point.f.6, point.f.7, point.f.8, point.f.9, point.f.10)
  interval.forecasts = list(point.interval.1, point.interval.2, point.interval.3, point.interval.4, point.interval.5, point.interval.6, point.interval.7, point.interval.8, point.interval.9, point.interval.10)
  
  return(list(point.forecasts, interval.forecasts))
}

preds_ext = get_predictions(1000, 20, A.posterior.draws, Sigma.posterior.draws)
preds_base = get_predictions(1000, 20, res[[1]], res[[2]])

```

The respective out of sample forecasts for 20 days ahead are plotted below. The forecasts from the baseline model are plotted in blue and the T-distributed error model forecasts are plotted in red. 90% confidence intervals are denoted by the shaded regions.

### 20 Day log RV Forecast Plots
```{r, echo=TRUE}

#Plotting function

plot_pred = function(ix, preds, preds2){
  h=length(preds[[1]][[1]])
  xdates = c(y$Date, seq(y[length(y[,1]),'Date'], by='day', length.out=h-p))
  
  index_name = colnames(Y_ext)[ix]
  plot(1:(length(Y_ext[,ix])+h), (c(Y_ext[,ix], preds[[1]][[ix]])), type='l', col='blue', ylab='', xlab='', main=paste(index_name,'log RV Forecasts'), xlim=c(4300,length(Y_ext[,ix])+h), xaxt='n', cex.axis=1, lwd=1)
  polygon(c((length(Y_ext[,ix])):(length(Y_ext[,ix])+h), (length(Y_ext[,ix])+h):(length(Y_ext[,ix]))), (c(Y_ext[length(Y_ext[,ix]),ix], preds[[2]][[ix]]['lower',], preds[[2]][[ix]]['upper',h:1], Y_ext[length(Y_ext[,ix]),ix])), col=rgb(0, 0, 1,0.1), border=NA)
  lines((c(Y_ext[,ix], preds2[[1]][[ix]])), type='l', col='red', ylab='', xlab='', main=paste(index_name,'log RV Forecasts'), xlim=c(4300,length(Y_ext[,ix])+h), xaxt='n', cex.axis=1, lwd=1)
  lines(Y_ext[,ix], type='l', col='blue')
  polygon(c((length(Y_ext[,ix])):(length(Y_ext[,ix])+h), (length(Y_ext[,ix])+h):(length(Y_ext[,ix]))), (c(Y_ext[length(Y_ext[,ix]),ix], preds2[[2]][[ix]]['lower',], preds2[[2]][[ix]]['upper',h:1], Y_ext[length(Y_ext[,ix]),ix])), col=rgb(1, 0, 0,0.1), border=NA)
  axis(1, at=1:(length(Y_ext[,ix])+h), labels=xdates, tick=FALSE, cex.axis=1)
}

```

```{r}

par(mfrow=c(2,2), mar=c(3,3,2,2))
plot_pred(1, preds_base, preds_ext)
plot_pred(2, preds_base, preds_ext)
plot_pred(3, preds_base, preds_ext)
plot_pred(4, preds_base, preds_ext)
plot_pred(5, preds_base, preds_ext)
plot_pred(6, preds_base, preds_ext)
plot_pred(7, preds_base, preds_ext)
plot_pred(8, preds_base, preds_ext)
plot_pred(9, preds_base, preds_ext)
plot_pred(10, preds_base, preds_ext)

```


As can be seen by the point forecasts, both model forecasts appear to converge back to a long term average. The forecasts trend higher over the forecast period for all variables, which might be explained as the model predicting volatility will normalise at a higher long run level after a period of relatively low volatility leading up to the forecast period. Forecasts in the T-distribution model appear to move back to the long run mean more quickly than that of the baseline model, suggesting faster mean reversion.

Comparing the confidence intervals of the two models, we can see that the T-distribution model has tighter 90% CIs for all forecasts. This may suggest that our assumption of T-distributed errors is appropriate. If the data has fatter tails than would be captured by a Normal distribution this may have been leading to larger variances estimated by the baseline model. The extended model allows for outliers in the data and therefore estimates the variance more accurately, leading to tighter intervals.

We can transform the variables and forecasts back into daily RVs by taking the exponent of the variables. Visually the forecasts seem reasonable given the historical data, with the forecasts again reverting towards the long term average. The confidence intervals also look sensible given the transformation back to levels, with the width of the band increasing as the forecasted RV increases as we would expect from the transformation. Again the T-Distribution model exhibits tighter CIs.

### 20 day RV Forecasts
```{r }

plot_pred_exp = function(ix, preds, preds2){
  h=length(preds[[1]][[1]])
  xdates = c(y$Date, seq(y[length(y[,1]),'Date'], by='day', length.out=h-p))
  
  index_name = colnames(Y_ext)[ix]
  plot(1:(length(Y_ext[,ix])+h), exp(c(Y_ext[,ix], preds[[1]][[ix]])), type='l', col='blue', ylab='', xlab='', ylim=c(0,0.0006), main=paste(index_name,'RV Forecasts'), xlim=c(4300,length(Y_ext[,ix])+h), xaxt='n', cex.axis=1, lwd=1)
  polygon(c((length(Y_ext[,ix])):(length(Y_ext[,ix])+h), (length(Y_ext[,ix])+h):(length(Y_ext[,ix]))), exp(c(Y_ext[length(Y_ext[,ix]),ix], preds[[2]][[ix]]['lower',], preds[[2]][[ix]]['upper',h:1], Y_ext[length(Y_ext[,ix]),ix])), col=rgb(0, 0, 1,0.1), border=NA)
  lines(exp(c(Y_ext[,ix], preds2[[1]][[ix]])), type='l', col='red', ylab='', xlab='', xlim=c(4300,length(Y_ext[,ix])+h), xaxt='n', cex.axis=1, lwd=1)
  lines(exp(Y_ext[,ix]), type='l', col='blue')
  polygon(c((length(Y_ext[,ix])):(length(Y_ext[,ix])+h), (length(Y_ext[,ix])+h):(length(Y_ext[,ix]))), exp(c(Y_ext[length(Y_ext[,ix]),ix], preds2[[2]][[ix]]['lower',], preds2[[2]][[ix]]['upper',h:1], Y_ext[length(Y_ext[,ix]),ix])), col=rgb(1, 0, 0,0.1), border=NA)
  axis(1, at=1:(length(Y_ext[,ix])+h), labels=xdates, tick=FALSE, cex.axis=1)
  legend("topright", legend=c("Baseline", "T-distribution"), col=c("blue", "red"), lty=1, cex=0.8)
}


par(mfrow=c(2,2), mar=c(3,3,2,2))
plot_pred_exp(1, preds_base, preds_ext)
plot_pred_exp(2, preds_base, preds_ext)
plot_pred_exp(3, preds_base, preds_ext)
plot_pred_exp(4, preds_base, preds_ext)
plot_pred_exp(5, preds_base, preds_ext)
plot_pred_exp(6, preds_base, preds_ext)
plot_pred_exp(7, preds_base, preds_ext)
plot_pred_exp(8, preds_base, preds_ext)
plot_pred_exp(9, preds_base, preds_ext)
plot_pred_exp(10, preds_base, preds_ext)

```

