[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Stock Index Realised Volatility using Bayesian VARs",
    "section": "",
    "text": "Abstract: This research report explores whether VARs and Bayesian VARs are able to predict realised volatility in equity index markets.\nKeywords. BVARs, Realised Volatility, Stock Indices, SP500"
  },
  {
    "objectID": "index.html#objective-and-motivation",
    "href": "index.html#objective-and-motivation",
    "title": "Predicting Stock Index Realised Volatility using Bayesian VARs",
    "section": "Objective and Motivation",
    "text": "Objective and Motivation\nThis paper seeks to examine the effectiveness of Bayesian VARs as a method for forecasting realised volatility (RV) in equity markets. It will explore how various Bayesian estimation techniques applied to vector autoregression perform when applied to RV. The accurate prediction of market volatility has many applications in Finance, including the pricing of derivatives and the estimation of risk measures such as Value at Risk."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Predicting Stock Index Realised Volatility using Bayesian VARs",
    "section": "Data",
    "text": "Data\nThe models will be applied to daily data on realised variances occurring on a group of global stock indices. Stock indices serve as gauges of overall equity market performance and are generally categorised by country. For this analysis we will focus on 10 major global indices, the SPX, DJI, FTSE, GDAXI, FCHI, STOXX50E, N225, AORD, HSI and the KS11.\n\n\n\nCode\nIndex Name\nRegion\n\n\n\n\nSPX\nS&P 500\nUS\n\n\nDJI\nDow Jones\nUS\n\n\nFTSE\nFTSE 100\nUK\n\n\nGDAXI\nDAX\nGermany\n\n\nFCHI\nCAC40\nFrance\n\n\nSTOXX50E\nEURO STOXX\nEurope\n\n\nN225\nNIKKEI 225\nJapan\n\n\nAORD\nAll Ordinaries\nAustralia\n\n\nHSI\nHang Seng\nHong Kong\n\n\nKS11\nKOSPI\nKorea\n\n\n\nRealised variance is a measure of historical volatility occurring in financial time series constructed from intraday high frequency return data.\nRealised variance is defined as the sum of squared returns over specific period:\n\\[\\begin{align}\nRealised\\,Variance = \\sum r^2_t\n\\end{align}\\]\nWhere \\(p_t\\) denotes the price of an asset at time t and \\(r_t\\) is defined as the log return over a predetermined interval, for example 5 minutes:\n\\[\\begin{align}\nr_t = log(p_t / p_{t-1})\n\\end{align}\\]\nRealised volatility RV is then computed as the square root of the realised variance.\n\\[\\begin{align}\nRV = \\sqrt{\\sum r^2_t}\n\\end{align}\\]\nThe RV data is sourced from the Oxford Man Realised Library which provides a number of precalculated volatility metrics, including RV on stock indices spanning multiple years. The data set has been employed widely in the literatur for empirical volatility studies, such as by Dutta and Das (2022) and Brandi and Di Matteo (2022). For this analysis we will utilise the 5 minute RV measure provided in the dataset for each of our 10 indices. The data ranges between January 2000 and June 2018.\n\nFigure 1. Time series plots of original values"
  },
  {
    "objectID": "index.html#model",
    "href": "index.html#model",
    "title": "Predicting Stock Index Realised Volatility using Bayesian VARs",
    "section": "Model",
    "text": "Model\nThe model will follow the standard \\(VAR(p)\\) setup as follows:\n\\[\\begin{align}\nrv_t &= \\mu_0 + A_1 rv_{t-1} + \\dots + A_p rv_{t-p} + \\epsilon_t\\\\\n\\epsilon_t | RV_{t-1} &\\sim iidN_N(0_N, \\Sigma)\n\\end{align}\\]\nWhere \\(rv_t\\) is a vector of log transformed realised variances for our \\(N=10\\) stock indices on day \\(t\\). The \\(A_i\\) matrices are \\(N\\times N\\) matrices of the autoregressive slope parameters.\nThe error term vector \\(\\epsilon_t\\) given the data up to \\(t-1\\) is assumed to be iid multivariate normally distributed of dimension \\(N=10\\), with mean \\(0_N\\) and covariance matrix \\(\\Sigma\\).\nBayesian estimation techniques will be then utilised in conjunction with a suitably chosen prior specification in order to estimate competing models and compute 20 day ahead RV forecasts across all indices.\nThe significance of being able to reliably forecast market volatility is primarily seen in the context of financial asset pricing. Volatility of the underlying asset is one of the crucial inputs required in options pricing. With reliable forecasts of stock index volatility one can assess the degree to which options quoted in the market are under or over estimating volatility relative to what is indicated by the historical realised dynamics."
  },
  {
    "objectID": "index.html#estimation-procedure-for-the-baseline-model",
    "href": "index.html#estimation-procedure-for-the-baseline-model",
    "title": "Predicting Stock Index Realised Volatility using Bayesian VARs",
    "section": "Estimation Procedure for the Baseline Model",
    "text": "Estimation Procedure for the Baseline Model\nThe baseline model is as follows:\n\\[\\begin{align}\nY &= XA + U \\\\\nY|X,A,\\Sigma &\\sim MN_{T \\times N} (XA, \\Sigma, I_T)\n\\end{align}\\]\nThis implies the following form for the kernel of the likelihood function:\n\\[\\begin{align}\nL(A,\\Sigma|Y,X) \\propto det(\\Sigma)^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr[\\Sigma^{-1}(Y-XA)'(Y-XA)])\n\\end{align}\\]\nWe assume the usual matrix normal and inverse Wishart natural conjugate priors for \\(A\\) and \\(\\Sigma\\):\n\\[\\begin{align}\np(A,\\Sigma) &= p(A|\\Sigma)p(\\Sigma) \\\\\nA|\\Sigma &\\sim MN_{K \\times N}(\\underline{A},\\Sigma,\\underline{V}) \\\\\n\\Sigma &\\sim IW_N(\\underline{S},\\underline{\\nu})\n\\end{align}\\]\nThe posterior distribution is given by the product of the likelihood and the priors.\n\\[\\begin{align}\np(A,\\Sigma|Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\\\\n&\\times exp(-\\frac{1}{2}tr[\\Sigma^{-1}(Y-XA)'(Y-XA)]) \\\\\n&\\times \\det(\\Sigma)^{-\\frac{N+K+\\underline{v}+1}{2}} \\\\\n&\\times exp(-\\frac{1}{2}tr[\\Sigma^{-1}(A-\\underline{A})'\\underline(V)^{-1}(A-\\underline{A})]) \\\\\n&\\times exp(-\\frac{1}{2}tr[\\Sigma^{-1}\\underline{S}])\n\\end{align}\\]\nCombining the terms and completing the squares for the terms within the square brackets yields the following posterior distributions for \\(A\\) and \\(\\Sigma\\):\n\\[\\begin{align}\np(A|Y,X,\\Sigma) &= MN_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V}) \\\\\np(\\Sigma|Y,X) &= IW_N(\\bar{S},\\bar{\\nu}) \\\\\n\\\\\n\\bar{V} &= (X'X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{\\nu} &= T + \\underline{\\nu} \\\\\n\\bar{S} &= \\underline{S} + Y'Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} - \\bar{A}'\\bar{V}^{-1}\\bar{A} \\\\\n\n\\end{align}\\]\nSince the above fully characterises the joint posterior distribution we can then sample from these directly using a suitably chosen prior. A common prior for unit root nonstationary variables is the ‘Minnesota Prior’, which assumes that the variables follow a random walk. The prior mean \\(\\underline{A}\\) is set to an identity matrix for the first lag and zeroes elsewhere. Meanwhile the column specific covariance matrix for A is set according to two shrinkage hyperparameters \\(\\kappa_1\\) and \\(\\kappa_2\\) which determine the level of shrinkage around the prior mean.\nAs the ADF tests for the log transformed RVs suggest that they are stationary, I instead work with an alternate form of the ‘Minnesota Prior’ whereby the variables are characterised as ‘white noise’. To achieve this, the \\(\\underline{A}\\) matrix is set to a \\(K \\times N\\) matrix of zeroes.\n\\[\\begin{align}\n\\underline{A} &= 0_{K \\times N} \\\\\n\n\\underline{V} &= diag( \\begin{bmatrix} \\kappa_2 & \\kappa_1(p^{-2}\\otimes I_N') \\end{bmatrix})\n\n\\end{align}\\]\nThe estimation routine can be implemented in a function in R as follows:\n\nPosterior Estimation Function in R\n\nget_posteriors = function(S, Y, X, priors) {\n  N = ncol(Y)\n  \n  A.prior     = priors[[1]]\n  V.prior     = priors[[2]]\n  S.prior     = priors[[3]]\n  nu.prior    = priors[[4]]\n    \n  # normal-inverse Wishart posterior parameters\n  ############################################################\n  V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))\n  V.bar       = solve(V.bar.inv)\n  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)\n  nu.bar      = nrow(Y) + nu.prior\n  S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n  S.bar.inv   = solve(S.bar)\n  \n  \n  # posterior draws \n  ############################################################\n  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)\n  Sigma.posterior   = apply(Sigma.posterior,3,solve)\n  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))\n  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))\n  L                 = t(chol(V.bar))\n  for (s in 1:S){\n    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])\n  }\n  \n  return(list(A.posterior, Sigma.posterior))\n}\n\nFor demonstrative purposes the above code can be applied to simulated data generated from a bivariate random walk.\n\n\n\n\n\nThe estimation routine utilising a Minnesota Prior specification is then implemented as follows:\n\np=1\nN=2\n\nY_ext = (rw_data[(p+1):nrow(rw_data),c(1,2)]) #removing first p observations and taking logs\nX_ext = matrix(1,nrow(Y_ext),1)                                       #Creating X matrix\nfor (i in 1:p){\n  X_ext     = cbind(X_ext, (rw_data[(p+1):nrow(rw_data)-i,c(1,2)]))\n}\n\nY_ext = as.matrix(Y_ext)\nX_ext = as.matrix(X_ext)\n\nA.hat       = solve(t(X_ext)%*%X_ext)%*%t(X_ext)%*%Y_ext\nSigma.hat   = t(Y_ext-X_ext%*%A.hat)%*%(Y_ext-X_ext%*%A.hat)/nrow(Y_ext)\n\nkappa.1     = 0.02^2\nkappa.2     = 100\nA.pri     = matrix(0, (1+p*N), N)\nA.pri[2:3,] = diag(2)\nV.pri     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))\nS.pri     = diag(diag(Sigma.hat))\nnu.pri    = N+1\n\npris = list(\n  A = A.pri, \n  V = V.pri, \n  s = S.pri, \n  nu = nu.pri\n)\n\nres = get_posteriors(10000, Y_ext, X_ext, pris)\n\nround(apply(res[[1]],1:2,mean),2) #A posterior mean\n\n      [,1]  [,2]\n[1,] -0.12 -0.08\n[2,]  1.00  0.01\n[3,]  0.00  1.00\n\nround(apply(res[[2]],1:2,mean),2) #Sigma posterior mean\n\n     [,1] [,2]\n[1,] 1.04 0.02\n[2,] 0.02 0.99\n\n\nAs shown above the estimated parameter means closely match the expected values for the data generating process."
  },
  {
    "objectID": "index.html#model-extension-t-distributed-errors",
    "href": "index.html#model-extension-t-distributed-errors",
    "title": "Predicting Stock Index Realised Volatility using Bayesian VARs",
    "section": "Model Extension: T-Distributed Errors",
    "text": "Model Extension: T-Distributed Errors\nAn alternative specification for the model is to relax the assumption of normally distributed errors. A T-Distribution more closely mirrors the leptokurtosis commonly seen in financial time series and as such it is a good candidate for our model of index volatility.\n\\[\\begin{align}\nU_t &\\sim t_N(0, \\Sigma, \\nu)\n\\end{align}\\]\nFollowing the methodology of Geweke 1993, a T-distribution for the error term can be represented by a scale mixture of normal distributions with a scaling term \\(\\lambda\\) which is Inverse Gamma 2 distributed.\n\\[\\begin{align}\nU|\\lambda &\\sim MN(0, \\Sigma, \\lambda I_T) \\\\\n\\lambda &\\sim IG2( s_{\\lambda}, \\nu_{\\lambda})\n\\end{align}\\]\nUnder this specification, the kernel of the likelihood function takes the following form:\n\\[\\begin{align}\n\nL(A,\\Sigma,\\Lambda|Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\lambda I_T)^{-\\frac{N}{2}} exp(-\\frac{1}{2} tr[\\Sigma^{-1} (Y-XA)' (\\lambda I_T)^{-1} (Y-XA) ])\n\n\\end{align}\\]\nThe posteriors for \\(A\\), \\(\\Sigma\\) and \\(\\lambda\\) can then be derived using the likelihood and the prior distributions. The natural conjugacy of \\(A\\) and \\(\\Sigma\\) is preserved and so the conditional posterior \\(p(A,\\Sigma|Y,X)\\) can be derived as follows.\n\\[\\begin{align}\np(A,\\Sigma|Y,X) &\\propto L(A,\\Sigma,\\lambda|Y,X)p(A,\\Sigma) \\\\\n\\\\\n&= \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\lambda I_T)^{-\\frac{N}{2}} exp(-\\frac{1}{2} tr[\\Sigma^{-1} (Y-XA)' (\\lambda I_T)^{-1} (Y-XA) ]) \\\\\n&\\times \\det(\\Sigma)^{-\\frac{N+k+\\underline{\\nu}+1}{2}} exp(-\\frac{1}{2}tr[\\Sigma^{-1}(A-\\underline{A})'(\\underline{V})^{-1}(A-\\underline{A})]) \\\\\n&\\times exp(-\\frac{1}{2}tr[\\Sigma^{-1}\\underline{S}]) \\\\\n\\\\\n&= \\det(\\Sigma)^{-\\frac{T+N+K+\\underline{\\nu}+1}{2}} \\det(\\lambda I_T)^{-\\frac{N}{2}} \\\\\n\n&\\times exp(-\\frac{1}{2} tr[\\Sigma^{-1}(Y'(\\lambda I_T)^{-1}Y - 2A'X'(\\lambda I_T)^{-1}Y + A'X'(\\lambda I_T)^{-1}XA + A'\\underline{V}^{-1}A -2A'\\underline{V}^{-1}\\underline{A} + \\underline{A}'\\underline{V}^{-1}\\underline{A} + \\underline{S})])\n\n\n\\end{align}\\]\nExpanding the terms inside the square brackets, followed by completing the squares, allows the above expression to be rearranged in the form of a Matrix-variate Normal Inverse Wishart kernel.\n\\[\\begin{align}\n\np(A,\\Sigma|Y,X) &\\sim MNIW(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu}) \\\\\n\\\\\n\\bar{V} &= (X'(\\lambda I_T)^{-1}X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'(\\lambda I_T)^{-1}Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{S} &= Y'(\\lambda I_T)^{-1}Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} + \\underline{S} - \\bar{A}'\\bar{V}^{-1}\\bar{A} \\\\\n\\bar{\\nu} &= T + \\underline{\\nu}\n\n\\end{align}\\]\nThe posterior distribution for \\(\\lambda\\) is then derived as follows:\n\\[\\begin{align}\n\np(\\lambda|Y,X,A,\\Sigma) &\\propto L(A,\\Sigma,\\lambda|Y,X)p(\\lambda) \\\\\n\\\\\n&= \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\lambda I_T)^{-\\frac{N}{2}} exp(-\\frac{1}{2} tr[\\Sigma^{-1} (Y-XA)' (\\lambda I_T)^{-1} (Y-XA) ]) \\\\\n&\\times \\lambda^{-\\frac{\\underline{\\nu_{\\lambda}}+2}{2}} exp(-\\frac{1}{2}\\frac{\\underline{s_{\\lambda}}}{\\lambda}) \\\\\n\\\\\n&= \\lambda^{-\\frac{TN}{2}} exp(-\\frac{1}{2}\\frac{1}{\\lambda} tr[\\Sigma^{-1}(Y-XA)'(Y-XA)]) \\\\\n&\\times \\lambda^{-\\frac{\\underline{\\nu_{\\lambda}}+2}{2}} exp(-\\frac{1}{2}\\frac{\\underline{s_{\\lambda}}}{\\lambda}) \\det(\\Sigma)^{-\\frac{T}{2}}\\det(I_T)^{-\\frac{N}{2}} \\\\\n\\\\\n&= \\lambda^{-\\frac{TN+\\underline{\\nu_{\\lambda}}+2}{2}} exp(-\\frac{1}{2}\\frac{1}{\\lambda} [tr[\\Sigma^{-1}(Y-XA)'(Y-XA)] +\\underline{s_{\\lambda}}]) \\det(\\Sigma)^{-\\frac{T}{2}} \\\\\n\\\\\n\\end{align}\\]\nAs such,\n\\[\\begin{align}\n\n\\lambda|Y,A,\\Sigma &\\sim IG2(\\bar{s_{\\lambda}},\\bar{\\nu_{\\lambda}}) \\\\\n\\bar{s_{\\lambda}} &= tr[\\Sigma^{-1}(Y-XA)'(Y-XA)] + \\underline{s_{\\lambda}} \\\\\n\\bar{\\nu_{\\lambda}} &= TN + \\underline{\\nu_{\\lambda}}\n\n\\end{align}\\]\nWe can then sample sequentially from the conditional posterior distributions via a Gibbs Sampler. In order to do so we initalise each of the parameters at arbitrary starting values. We then proceed accordingly:\n\nDraw \\(\\Sigma^{(i)}\\) from the \\(IW(\\bar{S},\\bar{\\nu})\\) distribution\nDraw \\(A^{(i)}\\) from the \\(MN(\\bar{A},\\Sigma^{(i)}, \\bar{V})\\) distribution\nDraw \\(\\lambda^{(i)}\\) from \\(IG2(\\bar{s_{\\lambda}},\\bar{\\nu_{\\lambda}})\\)\n\nWe repeat the above steps for the desired amount of iterations and collect the parameter draws in an array. We can then scrutinise the characteristics of the posterior densities using these draws.\n\nGibbs Sampler code for t-distributed errors:\n\nGS_tdistribution <- function(S, Y_ext, X_ext, priors) {\n  # Set priors\n  A.gprior = priors$A\n  A_V.gprior = priors$V\n  Sigma_s.gprior = priors$s\n  Sigma_v.gprior = priors$nu\n  lambda_s.gprior = priors$lambda_s\n  lambda_v.gprior = priors$lambda_nu\n\n  lambda.draw = lambda_s.gprior/rchisq(1, lambda_v.gprior)\n  \n  # Initialize arrays to store posterior draws\n  Sigma.posterior.draws = array(NA, c(N,N,S))\n  A.posterior.draws = array(NA, c((1+p*N),N,S))\n  lambda.posterior.draws = rep(NA,S)\n\n  # Loop over S iterations\n  for (s in 1:S){\n    lambda.gprior.diag = diag(lambda.draw, nrow(Y_ext))\n    \n    A_V.posterior     = solve(t(X_ext)%*%diag(1/diag(lambda.gprior.diag))%*%X_ext + solve(A_V.gprior))\n    A.posterior       = A_V.posterior%*%(t(X_ext)%*%diag(1/diag(lambda.gprior.diag))%*%Y_ext + solve(A_V.gprior)%*%A.gprior)\n    Sigma_s.posterior = t(Y_ext)%*%diag(1/diag(lambda.gprior.diag))%*%Y_ext + t(A.gprior)%*%solve(A_V.gprior)%*%A.gprior + Sigma_s.gprior - t(A.posterior)%*%solve(A_V.posterior)%*%A.posterior\n    Sigma_v.posterior = nrow(Y_ext) + Sigma_v.gprior\n    \n    Sigma.inv.draw      = rWishart(1, Sigma_v.posterior, solve(Sigma_s.posterior))[,,1]\n    Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)\n    \n    A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.posterior), sigma=Sigma.posterior.draws[,,s]%x%A_V.posterior), ncol=N)\n    \n    lambda_s.posterior = sum(diag(Sigma.inv.draw%*%t(Y_ext - X_ext%*%A.posterior.draws[,,s])%*%(Y_ext - X_ext%*%A.posterior.draws[,,s]))) + lambda_s.gprior\n    lambda_v.posterior = nrow(Y_ext)*2 + lambda_v.gprior\n    \n    lambda.draw = lambda_s.posterior / rchisq(1, lambda_v.posterior)\n    lambda.posterior.draws[s] = lambda.draw\n  }\n  return(list(Sigma.posterior.draws = Sigma.posterior.draws, \n              A.posterior.draws = A.posterior.draws, \n              lambda.posterior.draws = lambda.posterior.draws))\n}\n\ntdist_res_rw = GS_tdistribution(1000, Y_ext, X_ext, c(pris, list(lambda_s=1000, lambda_nu=1000)))\n\n# A posterior mean\nround(apply(tdist_res_rw$A.posterior.draws, 1:2, mean),2)\n\n      [,1]  [,2]\n[1,] -0.12 -0.08\n[2,]  1.00  0.01\n[3,]  0.00  1.00\n\n# Sigma posterior mean\nround(apply(tdist_res_rw$Sigma.posterior.draws, 1:2, mean),2)\n\n     [,1] [,2]\n[1,] 1.03 0.02\n[2,] 0.02 0.98\n\n# Lambda posterior mean\nround(mean(tdist_res_rw$lambda.posterior.draws),2)\n\n[1] 1.01\n\n\nAs can be seen, when the extended model estimation routine is applied to the simulated bivariate random walk data, the estimates of \\(A\\) and \\(\\Sigma\\) again match the true parameter values closely."
  },
  {
    "objectID": "index.html#model-extension-stochastic-volatility-with-t-distributed-errors",
    "href": "index.html#model-extension-stochastic-volatility-with-t-distributed-errors",
    "title": "Predicting Stock Index Realised Volatility using Bayesian VARs",
    "section": "Model Extension: Stochastic Volatility with T Distributed Errors",
    "text": "Model Extension: Stochastic Volatility with T Distributed Errors\nThe second extension to the model is a further augmentation to the form of the errors in order to account for stochastic volatility. We can explicitly account for heteroskedasticity in the errors by applying a model specification in which the variance changes over time according to some stochastic process. This extension can be combined with the previous model including t-distributed errors to achieve an even more robust model for errors.\n\\[ \\begin{align}\nY &= XA + U \\\\\nU|\\lambda &\\sim MN(0, \\Sigma, \\lambda \\text{diag}(\\sigma^2)) \\\\\n\\lambda &\\sim IG2( s_{\\lambda}, \\nu_{\\lambda}) \\\\\n\\\\\n\\sigma^2 &= (\\exp(h_1), ..., exp(h_T)) \\\\\nh_T &- \\text{follows a stochastic volatility process} \\\\\n\\lambda &- \\text{the scale parameter for t errors}\n\\end{align}\\]\nIt is convenient for estimation to assume \\(h_t\\) follows a random walk process. That is,\n\\[\n\\begin{align}\nh_t &= h_{t-1} + \\sigma_v v_t \\\\\n\\\\\nv_t &\\sim \\mathcal{N}(0,1) \\\\\n\\sigma^2_v &- \\text{estimated parameter of the model}\n\\end{align}\n\\]\nEstimation of \\(h\\) is completed via its own Gibbs Sampling routine. The sampler is applied to a log-linearised form of the data which strips out the conditional mean in order to isolate the error term. The sampling routine involves drawing estimated parameters from a combination of Normal, IG2 as well the log Chi-Square distribution. The log Chi-Square distribution in this case is approximated by a mixture of ten normal distributions and sampled from using the inverse transform method. One pass of the sampler draws a sample of all of the estimated parameters, including \\(T\\times1\\) vector \\(h\\), the exponent of which forms \\(\\sigma^2\\).\nWith the overall model specified in this form, the likelihood function is as follows, with the \\(\\sigma^2\\) diagonal matrix entering in place of the previous identity matrix \\(I_T\\).\n\\[\n\\begin{align}\nL(A,\\Sigma,\\Lambda|Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\lambda \\times \\text{diag}(\\sigma^2))^{-\\frac{N}{2}} exp(-\\frac{1}{2} tr[\\Sigma^{-1} (Y-XA)' (\\lambda \\times \\text{diag}(\\sigma^2))^{-1} (Y-XA) ])\n\\end{align}\n\\]\nFollowing the same derivations as before, we compute the full conditionals for \\(A\\), \\(\\Sigma\\) and \\(\\lambda\\) as\n\\[\n\\begin{align}\np(A,\\Sigma|Y,X) &= MNIW(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu}) \\\\\n\\\\\n\\bar{V} &= (X'(\\lambda  \\text{diag}(\\sigma^2))^{-1}X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'(\\lambda  \\text{diag}(\\sigma^2))^{-1}Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{S} &= Y'(\\lambda  \\text{diag}(\\sigma^2))^{-1}Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} + \\underline{S} - \\bar{A}'\\bar{V}^{-1}\\bar{A} \\\\\n\\bar{\\nu} &= T + \\underline{\\nu}\n\\\\\n\\\\\np(\\lambda|Y,A,\\Sigma) &= IG2(\\bar{s_{\\lambda}},\\bar{\\nu_{\\lambda}}) \\\\\n\\bar{s_{\\lambda}} &= tr[\\Sigma^{-1}(Y-XA)'\\text{diag}(\\sigma^2)^{-1}(Y-XA)] + \\underline{s_{\\lambda}} \\\\\n\\bar{\\nu_{\\lambda}} &= TN + \\underline{\\nu_{\\lambda}}\n\\end{align}\n\\]\n\nGibbs Sampling Routine for SV and T Distributed Errors\nInitialize \\(\\lambda^{(0)}\\) and \\(h_t^{(0)}\\).\nAt each iteration:\n\nDraw \\(\\Sigma^{(i)}\\) from the \\(IW(\\bar{S},\\bar{\\nu})\\) distribution\nDraw \\(A^{(i)}\\) from the \\(MN(\\bar{A},\\Sigma^{(i)}, \\bar{V})\\) distribution\nDraw \\(\\lambda^{(i)}\\) from \\(IG2(\\bar{s_{\\lambda}},\\bar{\\nu_{\\lambda}})\\)\nDraw \\(h^{(i)}\\) from the SV sampling routine described above\n\n\n\nGibbs Sampler Code\n\nSVcommon.Gibbs.iteration = function(aux, priors){\n  # A single iteration of the Gibbs sampler for the SV component\n  #\n  # aux is a list containing:\n  #   Y - a TxN matrix\n  #   X - a TxK matrix\n  #   H - a Tx1 matrix\n  #   h0 - a scalar\n  #   sigma.v2 - a scalar\n  #   s - a Tx1 matrix\n  #   A - a KxN matrix\n  #   Sigma - an NxN matrix\n  #   sigma2 - a Tx1 matrix\n  #\n  # priors is a list containing:\n  #   h0.v - a positive scalar\n  #   h0.m - a scalar\n  #   sigmav.s - a positive scalar\n  #   sigmav.nu - a positive scalar\n  #   HH - a TxT matrix\n  \n  T             = dim(aux$Y)[1]\n  N             = dim(aux$Y)[2]\n  alpha.st      = c(1.92677,1.34744,0.73504,0.02266,0-0.85173,-1.97278,-3.46788,-5.55246,-8.68384,-14.65000)\n  sigma.st      = c(0.11265,0.17788,0.26768,0.40611,0.62699,0.98583,1.57469,2.54498,4.16591,7.33342)\n  pi.st         = c(0.00609,0.04775,0.13057,0.20674,0.22715,0.18842,0.12047,0.05591,0.01575,0.00115)\n  \n  Lambda        = solve(chol(aux$Sigma))\n  Z             = rowSums( ( aux$Y - aux$X %*% aux$A ) %*% Lambda ) / sqrt(N)\n  Y.tilde       = as.vector(log((Z + 0.0000001)^2))\n  Ytilde.alpha  = as.matrix(Y.tilde - alpha.st[as.vector(aux$s)])\n  \n  # sampling initial condition\n  ############################################################\n  V.h0.bar      = 1/((1 / priors$h0.v) + (1 / aux$sigma.v2))\n  m.h0.bar      = V.h0.bar*((priors$h0.m / priors$h0.v) + (aux$H[1] / aux$sigma.v2))\n  h0.draw       = rnorm(1, mean = m.h0.bar, sd = sqrt(V.h0.bar))\n  aux$h0        = h0.draw\n  \n  # sampling sigma.v2\n  ############################################################\n  sigma.v2.s    = priors$sigmav.s + sum(c(aux$H[1] - aux$h0, diff(aux$H))^2)\n  sigma.v2.draw = sigma.v2.s / rchisq(1, priors$sigmav.nu + T)\n  aux$sigma.v2  = sigma.v2.draw\n  \n  # sampling auxiliary states\n  ############################################################\n  Pr.tmp        = simplify2array(lapply(1:10,function(x){\n    dnorm(Y.tilde, mean = as.vector(aux$H + alpha.st[x]), sd = sqrt(sigma.st[x]), log = TRUE) + log(pi.st[x])\n  }))\n  Pr            = t(apply(Pr.tmp, 1, function(x){exp(x - max(x)) / sum(exp(x - max(x)))}))\n  s.cum         = t(apply(Pr, 1, cumsum))\n  r             = matrix(rep(runif(T), 10), ncol = 10)\n  ss            = apply(s.cum < r, 1, sum) + 1\n  aux$s         = as.matrix(ss)\n  \n  \n  # sampling log-volatilities using functions for tridiagonal precision matrix\n  ############################################################\n  Sigma.s.inv   = diag(1 / sigma.st[as.vector(aux$s)])\n  D.inv         = Sigma.s.inv + (1 / aux$sigma.v2) * priors$HH\n  b             = as.matrix(Ytilde.alpha / sigma.st[as.vector(aux$s)] + (aux$h0/aux$sigma.v2)*diag(T)[,1])\n  lead.diag     = diag(D.inv)\n  sub.diag      = mgcv::sdiag(D.inv, -1)\n  D.chol        = mgcv::trichol(ld = lead.diag, sd = sub.diag)\n  D.L           = diag(D.chol$ld)\n  mgcv::sdiag(D.L,-1) = D.chol$sd\n  x             = as.matrix(rnorm(T))\n  a             = forwardsolve(D.L, b)\n  draw          = backsolve(t(D.L), a + x)\n  aux$H         = as.matrix(draw)\n  aux$sigma2    = as.matrix(exp(draw))\n  \n  return(aux)\n}\n\n\nGS_sv_tdist = function(S, Y_ext, X_ext, priors_sv){\n  #Full estimation routine for SV model\n  \n  aux_sv = list(\n    Y = Y_ext,\n    X = X_ext,\n    H = matrix(1, nrow(Y_ext), 1),\n    h0 = 0,\n    sigma.v2 = 1,\n    s = matrix(1, nrow(Y_ext), 1),\n    A = matrix(0, (1+p*N), N),\n    Sigma = diag(diag(matrix(1,N,N))),\n    sigma2 = matrix(1,nrow(Y_ext),1)\n  )\n  \n  posteriors      = list(                        \n    H           = matrix(NA,nrow(Y_ext),S),\n    sigma2      = matrix(NA,nrow(Y_ext),S),\n    s           = matrix(NA,nrow(Y_ext),S),\n    h0          = rep(NA,S),\n    sigma.v2    = rep(NA,S),\n    A           = array(NA, c((1+p*N),N,S)),\n    Sigma       = array(NA, c(N,N,S)),\n    lambda       = rep(NA, S)\n  )\n  \n  lambda.draw = priors_sv$lambda_s/rchisq(1, priors_sv$lambda_v)\n  \n  # Sigma.posterior.draws = array(NA, c(N,N,S))\n  # A.posterior.draws = array(NA, c((1+p*N),N,S))\n  lambda.posterior.draws = rep(NA,S)\n  \n  for (s in 1:S){\n\n    lambda.gprior.diag = lambda.draw * matrix(diag(as.vector(aux_sv$sigma2)), ncol = nrow(Y_ext))\n    \n    A_V.posterior     = solve(t(X_ext)%*%diag(1/diag(lambda.gprior.diag))%*%X_ext + solve(priors_sv$V))\n    A.posterior       = A_V.posterior%*%(t(X_ext)%*%diag(1/diag(lambda.gprior.diag))%*%Y_ext + solve(priors_sv$V)%*%priors_sv$A)\n    Sigma_s.posterior = t(Y_ext)%*%diag(1/diag(lambda.gprior.diag))%*%Y_ext + t(priors_sv$A)%*%solve(priors_sv$V)%*%priors_sv$A + priors_sv$S - t(A.posterior)%*%solve(A_V.posterior)%*%A.posterior\n    Sigma_v.posterior = nrow(Y_ext) + priors_sv$nu\n    \n    Sigma.inv.draw      = rWishart(1, Sigma_v.posterior, solve(Sigma_s.posterior))[,,1]\n    aux_sv$Sigma = solve(Sigma.inv.draw)\n    \n    aux_sv$A = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.posterior), sigma=aux_sv$Sigma%x%A_V.posterior), ncol=N)\n    \n    lambda_s.posterior = sum(diag(Sigma.inv.draw%*%t(Y_ext - X_ext%*%aux_sv$A)%*%diag(1/diag(matrix(diag(as.vector(aux_sv$sigma2)), ncol = nrow(Y_ext))))%*%(Y_ext - X_ext%*%aux_sv$A))) + priors_sv$lambda_s\n    lambda_v.posterior = nrow(Y_ext)*2 + priors_sv$lambda_v\n    \n    lambda.draw = lambda_s.posterior / rchisq(1, lambda_v.posterior)\n    lambda.posterior.draws[s] = lambda.draw\n    \n    aux_sv = SVcommon.Gibbs.iteration(aux_sv, priors_sv)\n    \n    posteriors$H[,s]             = aux_sv$H\n    posteriors$sigma2[,s]        = aux_sv$sigma2\n    posteriors$s[,s]             = aux_sv$s\n    posteriors$h0[s]             = aux_sv$h0\n    posteriors$sigma.v2[s]       = aux_sv$sigma.v2\n    posteriors$A[,,s]            = aux_sv$A\n    posteriors$Sigma[,,s]        = aux_sv$Sigma\n    posteriors$lambda[s]         = lambda.draw\n  }\n  return(posteriors)\n}\n\n\nHH            = 2*diag(nrow(Y_ext))\nmgcv::sdiag(HH,-1)  =  -1\nmgcv::sdiag(HH,1)   =  -1\n\npriors_sv = list(\n  A = matrix(0, (1+p*N), N),\n  V = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),\n  S = diag(diag(Sigma.hat)),\n  nu = diag(diag(Sigma.hat)),\n  lambda_s = 1000,\n  lambda_v = 1000,\n  HH = HH,\n  h0.m     = 0,\n  h0.v     = 1,\n  sigmav.s = 1,\n  sigmav.nu= 1\n)\n\nsvres = GS_sv_tdist(1000, Y_ext, X_ext, priors_sv)\n\nhead(round(apply(svres$A, 1:2, mean),2))\n\n      [,1]  [,2]\n[1,] -0.15 -0.14\n[2,]  0.99  0.01\n[3,]  0.00  0.99\n\nhead(round(apply(svres$Sigma, 1:2, mean),2))\n\n      [,1]  [,2]\n[1,]  7.24 -0.23\n[2,] -0.23  7.10\n\nround(mean(svres$lambda),2)\n\n[1] 0.59\n\nround(mean(svres$sigma2),2)\n\n[1] 0.54"
  },
  {
    "objectID": "index.html#concluding-remarks",
    "href": "index.html#concluding-remarks",
    "title": "Predicting Stock Index Realised Volatility using Bayesian VARs",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nAs illustrated in the above plots,"
  }
]